{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "oxg93PeBwRn1"
      },
      "outputs": [],
      "source": [
        "# === IMPORTS === #\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import joblib\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === DEVICE === #\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "kAydG1H6w0Yg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === LOAD DATASET === #\n",
        "df = pd.read_parquet(\"clinvar_dmd_embeddings.parquet\")"
      ],
      "metadata": {
        "id": "bI9Glirmw0wZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SAVE LABEL ENCODERS FOR STRUCTURED CATEGORICAL === #\n",
        "for col in [\"Variant type\"]:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = df[col].fillna(\"unknown\").astype(str)\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    joblib.dump(le, f\"label_encoder_{col.replace(' ', '_')}.pkl\")"
      ],
      "metadata": {
        "id": "GiW1qKwzw8ax"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENCODE TARGET === #\n",
        "le_target = LabelEncoder()\n",
        "df[\"Germline classification\"] = df[\"Germline classification\"].astype(str)\n",
        "df[\"target\"] = le_target.fit_transform(df[\"Germline classification\"])\n",
        "joblib.dump(le_target, \"label_encoder_germline.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUglaX6rxX-1",
        "outputId": "4f368572-3c21-4059-bbff-22a1be5c0b4a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['label_encoder_germline.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === EMBEDDINGS === #\n",
        "llm_features = [col for col in df.columns if \"_llm_\" in col]\n",
        "X_llm = df[llm_features].values.astype(np.float32)"
      ],
      "metadata": {
        "id": "WG2_9nhIxaAy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === STRUCTURED FEATURES === #\n",
        "structured_cols = ['Start_Pos', 'End_Pos', 'Variant_Length', 'has_fs', 'has_stop', 'n_protein_changes', 'Variant type']\n",
        "X_struct = df[structured_cols].fillna(0).values.astype(np.float32)"
      ],
      "metadata": {
        "id": "67_fcEM-xcC6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === LIST-BASED FEATURES === #\n",
        "from collections import defaultdict\n",
        "\n",
        "def fit_label_encoder_for_lists(column):\n",
        "    all_items = set(item for sublist in df[column] for item in sublist)\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(all_items) + [\"unknown\"])\n",
        "    joblib.dump(le, f\"label_encoder_{column}.pkl\")\n",
        "    return le\n",
        "\n",
        "def list_to_index_tensor(list_col, le, max_len):\n",
        "    index_seqs = [torch.tensor([le.transform([item])[0] if item in le.classes_ else le.transform([\"unknown\"])[0] for item in row]) for row in list_col]\n",
        "    padded = pad_sequence(index_seqs, batch_first=True, padding_value=0)\n",
        "    return padded[:, :max_len]\n",
        "\n",
        "# Fit encoders\n",
        "le_gene = fit_label_encoder_for_lists(\"Gene_list\")\n",
        "le_cond = fit_label_encoder_for_lists(\"Condition_list\")\n",
        "le_cons = fit_label_encoder_for_lists(\"Consequence_list\")\n",
        "le_allele = fit_label_encoder_for_lists(\"Allele_list\")\n",
        "\n",
        "# Convert to tensors\n",
        "gene_tensor = list_to_index_tensor(df['Gene_list'], le_gene, 10)\n",
        "cond_tensor = list_to_index_tensor(df['Condition_list'], le_cond, 5)\n",
        "cons_tensor = list_to_index_tensor(df['Consequence_list'], le_cons, 5)\n",
        "allele_tensor = list_to_index_tensor(df['Allele_list'], le_allele, 10)\n",
        "\n",
        "target = df['target'].values"
      ],
      "metadata": {
        "id": "iJQa9BA9xgoP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === COMPUTE CLASS WEIGHTS === #\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(target),\n",
        "    y=target\n",
        ")\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n"
      ],
      "metadata": {
        "id": "BNHuZWWs4kUm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === DEFINE MODEL === #\n",
        "class DMDMultiInputModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 llm_dim=768*4,\n",
        "                 structured_dim=7,\n",
        "                 gene_vocab_size=2640,\n",
        "                 cond_vocab_size=80,\n",
        "                 cons_vocab_size=20,\n",
        "                 allele_vocab_size=10325,\n",
        "                 output_dim=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gene_embed = nn.Embedding(gene_vocab_size, 64, padding_idx=0)\n",
        "        self.cond_embed = nn.Embedding(cond_vocab_size, 32, padding_idx=0)\n",
        "        self.cons_embed = nn.Embedding(cons_vocab_size, 16, padding_idx=0)\n",
        "        self.allele_embed = nn.Embedding(allele_vocab_size, 64, padding_idx=0)\n",
        "\n",
        "        self.llm_fc = nn.Linear(llm_dim, 256)\n",
        "        self.struct_fc = nn.Linear(structured_dim, 64)\n",
        "\n",
        "        self.final_fc = nn.Sequential(\n",
        "            nn.Linear(256 + 64 + 64 + 32 + 16 + 64, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, llm_x, struct_x, gene_ids, cond_ids, cons_ids, allele_ids):\n",
        "        gene_vec = self.gene_embed(gene_ids).mean(dim=1)\n",
        "        cond_vec = self.cond_embed(cond_ids).mean(dim=1)\n",
        "        cons_vec = self.cons_embed(cons_ids).mean(dim=1)\n",
        "        allele_vec = self.allele_embed(allele_ids).mean(dim=1)\n",
        "\n",
        "        llm_feat = F.relu(self.llm_fc(llm_x))\n",
        "        struct_feat = F.relu(self.struct_fc(struct_x))\n",
        "\n",
        "        x = torch.cat([llm_feat, struct_feat, gene_vec, cond_vec, cons_vec, allele_vec], dim=1)\n",
        "        return self.final_fc(x)"
      ],
      "metadata": {
        "id": "xsadr6Shxjrd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === TRAIN MODEL === #\n",
        "idx_train, idx_test = train_test_split(\n",
        "    np.arange(len(df)), test_size=0.2, stratify=target, random_state=42)\n",
        "\n",
        "model = DMDMultiInputModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    permutation = torch.randperm(len(idx_train))\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(permutation), BATCH_SIZE):\n",
        "        batch_indices = permutation[i:i + BATCH_SIZE]\n",
        "        batch_ids = idx_train[batch_indices]  # this gives real data row indices\n",
        "\n",
        "        batch_llm = torch.tensor(X_llm[batch_ids]).to(device)\n",
        "        batch_struct = torch.tensor(X_struct[batch_ids]).to(device)\n",
        "        batch_gene = gene_tensor[batch_ids].to(device)\n",
        "        batch_cond = cond_tensor[batch_ids].to(device)\n",
        "        batch_cons = cons_tensor[batch_ids].to(device)\n",
        "        batch_allele = allele_tensor[batch_ids].to(device)\n",
        "        batch_target = torch.tensor(target[batch_ids]).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_llm, batch_struct, batch_gene, batch_cond, batch_cons, batch_allele)\n",
        "        loss = loss_fn(output, batch_target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "    if epoch == 0:\n",
        "        print(\"Sample target batch:\", batch_target[:10])\n",
        "        print(\"Sample output:\", output[:2].cpu().detach().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO3N-WRFxoHL",
        "outputId": "74a7d056-b773-415c-c3fa-c3449bc4d9bc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: 204.5886\n",
            "Sample target batch: tensor([7, 7])\n",
            "Sample output: [[-1.665524    1.2894893   1.012018    0.04047731 -1.6004074  -3.1071808\n",
            "  -2.14112     2.0404668 ]\n",
            " [-1.6721618   0.24505603  0.18369399 -0.46111047 -1.5689144  -1.1376799\n",
            "  -1.6126863   2.0893607 ]]\n",
            "Epoch 2/15, Loss: 153.6431\n",
            "Epoch 3/15, Loss: 130.0872\n",
            "Epoch 4/15, Loss: 116.7089\n",
            "Epoch 5/15, Loss: 104.1498\n",
            "Epoch 6/15, Loss: 96.8221\n",
            "Epoch 7/15, Loss: 87.8316\n",
            "Epoch 8/15, Loss: 79.9915\n",
            "Epoch 9/15, Loss: 70.7795\n",
            "Epoch 10/15, Loss: 60.2807\n",
            "Epoch 11/15, Loss: 51.7645\n",
            "Epoch 12/15, Loss: 46.0761\n",
            "Epoch 13/15, Loss: 38.8717\n",
            "Epoch 14/15, Loss: 30.5196\n",
            "Epoch 15/15, Loss: 26.3582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUATE === #\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    batch_llm = torch.tensor(X_llm[idx_test]).to(device)\n",
        "    batch_struct = torch.tensor(X_struct[idx_test]).to(device)\n",
        "    batch_gene = gene_tensor[idx_test].to(device)\n",
        "    batch_cond = cond_tensor[idx_test].to(device)\n",
        "    batch_cons = cons_tensor[idx_test].to(device)\n",
        "    batch_allele = allele_tensor[idx_test].to(device)\n",
        "    batch_target = torch.tensor(target[idx_test]).to(device)\n",
        "\n",
        "    output = model(batch_llm, batch_struct, batch_gene, batch_cond, batch_cons, batch_allele)\n",
        "    pred = torch.argmax(output, dim=1).cpu().numpy()\n",
        "    y_true = batch_target.cpu().numpy()\n",
        "# Compute all expected labels\n",
        "all_labels = list(range(len(le_target.classes_)))\n",
        "\n",
        "# Safe classification report\n",
        "print(classification_report(\n",
        "    y_true, pred,\n",
        "    labels=all_labels,\n",
        "    target_names=le_target.classes_,\n",
        "    zero_division=0\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwFpPK_rxqOh",
        "outputId": "99af79c0-0951-483b-93bc-e1cdea8256af"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              precision    recall  f1-score   support\n",
            "\n",
            "                                      Benign       0.38      0.54      0.45        96\n",
            "                        Benign/Likely benign       0.15      0.29      0.20        31\n",
            "Conflicting classifications of pathogenicity       0.48      0.59      0.53       128\n",
            "                               Likely benign       0.79      0.78      0.78       611\n",
            "                           Likely pathogenic       0.22      0.42      0.29       101\n",
            "                                  Pathogenic       0.81      0.73      0.77       542\n",
            "                Pathogenic/Likely pathogenic       0.10      0.22      0.13        23\n",
            "                      Uncertain significance       0.85      0.62      0.72       533\n",
            "\n",
            "                                    accuracy                           0.67      2065\n",
            "                                   macro avg       0.47      0.52      0.48      2065\n",
            "                                weighted avg       0.73      0.67      0.69      2065\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === SAVE MODEL === #\n",
        "torch.save(model.state_dict(), \"latest_dmd_model.pt\")"
      ],
      "metadata": {
        "id": "2bTefbh8xsNS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === LOAD & DEMO PREDICTION === #\n",
        "# Load encoders\n",
        "le_target = joblib.load(\"label_encoder_germline.pkl\")\n",
        "le_gene = joblib.load(\"label_encoder_Gene_list.pkl\")\n",
        "le_cond = joblib.load(\"label_encoder_Condition_list.pkl\")\n",
        "le_cons = joblib.load(\"label_encoder_Consequence_list.pkl\")\n",
        "le_allele = joblib.load(\"label_encoder_Allele_list.pkl\")\n",
        "le_vtype = joblib.load(\"label_encoder_Variant_type.pkl\")\n",
        "# Load model\n",
        "model = DMDMultiInputModel().to(device)\n",
        "model.load_state_dict(torch.load(\"best_dmd_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# Example sample for testing\n",
        "def prepare_single_sample(sample_dict):\n",
        "    llm_feat = torch.tensor(sample_dict['llm'], dtype=torch.float).unsqueeze(0).to(device)\n",
        "\n",
        "    struct_feat = torch.tensor(sample_dict['structured'], dtype=torch.float).unsqueeze(0).to(device)\n",
        "    gene_ids = list_to_index_tensor([sample_dict['Gene_list']], le_gene, 10).to(device)\n",
        "    cond_ids = list_to_index_tensor([sample_dict['Condition_list']], le_cond, 5).to(device)\n",
        "    cons_ids = list_to_index_tensor([sample_dict['Consequence_list']], le_cons, 5).to(device)\n",
        "    allele_ids = list_to_index_tensor([sample_dict['Allele_list']], le_allele, 10).to(device)\n",
        "    return llm_feat, struct_feat, gene_ids, cond_ids, cons_ids, allele_ids\n",
        "\n",
        "# Sample prediction input\n",
        "sample_input = {\n",
        "    'llm': X_llm[0],\n",
        "    'structured': X_struct[0],\n",
        "    'Gene_list': df['Gene_list'].iloc[0],\n",
        "    'Condition_list': df['Condition_list'].iloc[0],\n",
        "    'Consequence_list': df['Consequence_list'].iloc[0],\n",
        "    'Allele_list': df['Allele_list'].iloc[0]\n",
        "}\n",
        "print(sample_input)\n",
        "llm_x, struct_x, gene_ids, cond_ids, cons_ids, allele_ids = prepare_single_sample(sample_input)\n",
        "with torch.no_grad():\n",
        "    pred = model(llm_x, struct_x, gene_ids, cond_ids, cons_ids, allele_ids)\n",
        "    label = le_target.inverse_transform([torch.argmax(pred).item()])[0]\n",
        "    print(\"Predicted Label:\", label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lttJxf61xuql",
        "outputId": "2b3a5189-2a40-49b8-ec68-916f801207dd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'llm': array([ 0.24032561,  0.2860818 ,  0.23787445, ..., -0.47412282,\n",
            "       -0.12398335, -0.52267736], dtype=float32), 'structured': array([0., 0., 0., 0., 0., 1., 2.], dtype=float32), 'Gene_list': array(['LOC130068431', 'LOC130068432', 'LOC130068480', ..., 'LINC01281',\n",
            "       'LINC01282', 'LINC01283'], dtype=object), 'Condition_list': array(['Autism', 'Schizophrenia'], dtype=object), 'Consequence_list': array(['missense variant'], dtype=object), 'Allele_list': array(['481029'], dtype=object)}\n",
            "Predicted Label: Pathogenic\n"
          ]
        }
      ]
    }
  ]
}